{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Deep Learning Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Julia?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this course we will work with [Julia](https://julialang.org/) language. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is a script language (just like Python or R)\n",
    "- Julia is fast (almost like C)\n",
    "- It have developed [type system](https://upload.wikimedia.org/wikipedia/commons/d/d9/Julia-number-type-hierarchy.svg) \n",
    "- Build-in functions for distributed computing and GPU usage.\n",
    "- Julia is easy to integrate with other languages (Python, R, C, …) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional materials "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Julia Academy](https://juliaacademy.com/)\n",
    "- [Boyd and Vandenberghe Linear Algebra book](http://vmls-book.stanford.edu/)\n",
    "- [Julia Express](https://github.com/bkamins/The-Julia-Express)\n",
    "- [Quantitative Economics lectures by Sargent and Stachursky](https://lectures.quantecon.org/jl/)\n",
    "- [Julia for Data Science](http://ucidatascienceinitiative.github.io/IntroToJulia/)\n",
    "- [Think Julia](https://benlauwens.github.io/ThinkJulia.jl/latest/book.html)\n",
    "- [Other materials avalaible at the language site](https://julialang.org/learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1. DataFrames.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<tt>DataFrames</tt>](https://dataframes.juliadata.org/stable/) is a package with resources for working with tabular data. It is an implementation of well-known data frames, with all the same tools as in the <tt>R</tt> or <tt>Pandas</tt> library in <tt>Python</tt>. If you want to learn more you could take a look at the [introduction to <tt>DataFrames</tt> ](https://github.com/bkamins/Julia-DataFrames-Tutorial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Plots.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Plots](https://docs.juliaplots.org/latest/tutorial/) is a basic tool for plotting in Julia. The biggest advantage of this package is an access to many different plotting [backends](http://docs.juliaplots.org/latest/backends/). Documentation of <tt>Plots.jl</tt> is avalaible [here](http://docs.juliaplots.org/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory  Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let start with a simple case study. We will build a basic perceptron for the classification task on the Australian credit scoring data (avalaible [here](http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/australian/australian.dat)). Our goal is to train a model to distinguish between the good and bad debtors. Obviously, we must start with importing the necessary packages and preparing a data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DelimitedFiles\n",
    "using PyPlot\n",
    "using Statistics\n",
    "using StatsBase\n",
    "using DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isfile(\"australian.dat\") ||\n",
    " download(\"http://archive.ics.uci.edu/ml/machine-learning-databases/\n",
    "    statlog/australian/australian.dat\")\n",
    "rawdata = readdlm(\"australian.dat\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrames.DataFrame(rawdata)\n",
    "rename!(df,:x15 => :class)\n",
    "df[!,:x4] = [x == 1 ? 1.0 : 0.0 for x in df[!,:x4]]\n",
    "df[!,:x12] = [x == 1 ? 1.0 : 0.0 for x in df[!,:x12]]\n",
    "df[!,:x14] = log.(df[!,:x14])\n",
    "first(df,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countmap(df[!, :class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = df[1:floor(Int,size(df,1)*train_ratio),:];\n",
    "test_set = df[floor(Int,size(df,1)*train_ratio + 1):end,:];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Matrix(train_set[:,1:end-1])';\n",
    "X_test = Matrix(test_set[:,1:end-1])';\n",
    "y_train = train_set[!, :class];\n",
    "y_test = test_set[!, :class];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data should be normalized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function scale(X)\n",
    "\n",
    "    μ = mean(X, dims=2)\n",
    "    σ = std(X, dims=2)\n",
    "\n",
    "    X_norm = (X .- μ) ./ σ\n",
    "\n",
    "    return (X_norm, μ, σ);\n",
    "end\n",
    "\n",
    "function scale(X, μ, σ)\n",
    "    X_norm = (X .- μ) ./ σ\n",
    "    return X_norm;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, μ, σ = scale(X_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scale(X_test, μ, σ);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we must define the weights of the model and the sigmoidal function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "β = rand(1,size(X_train,1) + 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predict(β, x) = 1 ./ (1 .+ exp.(-β[1:end-1]' * x .- β[end]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predict(β,X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Binary cross-entropy </b> (also known as <b>log-loss</b>) is the loss function we will use. It is defined as:\n",
    "\n",
    "$$ H_p(q) = - \\sum_{i=1}^N {y_i log(p(y_i)) + (1 - y_i) log(p(1 -y_i))}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L(ŷ, y) = (-y') * log.(ŷ') - (1 .- y') * log.(1 .- ŷ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize the weights $\\beta$ we will use the simple [gradient descent method](https://en.wikipedia.org/wiki/Gradient_descent):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function simple_∇(β, X, y)\n",
    "    J = L(Predict(β, X),y)[1] \n",
    "    ∇ = Float64[]\n",
    "    for i = 1:length(β)\n",
    "        b = β[i]\n",
    "        β′ = β .+ (LinearIndices(β) .== i) * b * √eps()\n",
    "        β′′ = β .- (LinearIndices(β) .== i) * b * √eps()\n",
    "        Δf = (L(Predict(β′,X),y)[1] - L(Predict(β′′,X),y)[1]) / (2*b*√eps())\n",
    "        push!(∇,Δf)\n",
    "    end\n",
    "    return J, ∇\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_∇(β,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function solve!(β, X, y;\n",
    "            η = 0.001, ϵ = 10^-10, maxit = 50_000)\n",
    "    iter = 1\n",
    "    Js = Float64[]\n",
    "    J, ∇ = simple_∇(β, X, y)\n",
    "    push!(Js,J)\n",
    "    while true\n",
    "        β₀ = β\n",
    "        β -= η * ∇'\n",
    "        J, ∇ = simple_∇(β, X, y)\n",
    "        push!(Js,J)\n",
    "        stop = maximum(abs.(β .- β₀))\n",
    "        stop < ϵ && break\n",
    "        iter += 1\n",
    "        iter > maxit && break\n",
    "    end\n",
    "    return Js\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Js = solve!(β,X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(Js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(β, X, y, T = 0.5) = sum((Predict(β, X)' .≥ T ).== y)/length(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(β, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously we will not define entire models in every case. Especially when models will became much more complicated than the simple perceptron. We will use [flux.jl](http://fluxml.ai/):\n",
    "\n",
    "- [Flux](http://fluxml.ai/) is a Julia machine learning stack.\n",
    "- Flux is lightweight, written entirely in Julia; it is trivial to hack it and build models suited for very specific cases. \n",
    "- Flux supports all the Julia's syntax; the vast majority of Julia's functions and macros could be used inside the model.\n",
    "- Building of basic models is very easy and intuitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Flux allows to define the layer on neural network in many different ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time using Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = rand(4, 8)\n",
    "b = rand(4)\n",
    "layer₁(x) = 1.0 ./ (1.0.+exp.(-W*x - b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rand(8)\n",
    "layer₁(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also use the already implemented, most popular [types of layers](https://fluxml.ai/Flux.jl/stable/models/layers/#Basic-Layers-1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer₂(x) = σ.(W * x .+ b)\n",
    "layer₂(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer₃ = Dense(8,4,σ)\n",
    "layer₃(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also define them on our own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Poly\n",
    "    W\n",
    "    V\n",
    "    b\n",
    "end\n",
    "\n",
    "Poly(in::Integer, out::Integer) =\n",
    "  Poly((randn(out, in)),randn(out, in), (randn(out)))\n",
    "\n",
    "# Overload call, so the object can be used as a function\n",
    "(m::Poly)(x) = m.W * x.^2 + m.V*x .+ m.b\n",
    "\n",
    "a = Poly(10, 5)\n",
    "\n",
    "a(rand(10)) # => 5-element vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, to use such user-defined layer in the Flux, we must use the macro <tt>@functor </tt>. Without that we will not be able to use all the built-in functions, e.g.  gradient propagation or [GPU computing](https://fluxml.ai/Flux.jl/stable/gpu/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.@functor  Poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu(a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, our model will have more than one layer. Again, there are plenty of methods to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer₁ = Dense(28^2, 32, relu)\n",
    "Layer₂ = Dense(32, 10)\n",
    "Layer₃ = softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function <tt>Chain</tt> allows us to merge multiple layers in the sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = Chain(x -> x^2, x-> -x)\n",
    "m₁ = Chain(Layer₁ , Layer₂, Layer₃) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also define the model as a function composition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m₂(x) = Layer₃(Layer₂(Layer₁(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m₃(x) = Layer₁ ∘ Layer₂ ∘ Layer₃  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m₄(x) = Layer₁(x) |> Layer₂  |> Layer₃ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function; Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Goodfellow I., Bengio Y., Courville A. (2016), Deep Learning, Chapter 7](http://www.deeplearningbook.org/contents/regularization.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it was mention in the last lecture, we cannot optimize the neural network directly; we must define and use a cost function $J(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could define it on our own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Dense(5,2)\n",
    "x, y = rand(5), rand(2);\n",
    "loss(ŷ, y) = sum((ŷ.- y).^2)/ length(y)\n",
    "loss(model(x), y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or use the one [implemented in Flux](https://github.com/FluxML/Flux.jl/blob/8f73dc6e148eedd11463571a0a8215fd87e7e05b/src/layers/stateless.jl):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.mse(model(x),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well trained model will have the smallest possible <b>generalization error</b>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](https://cdn-images-1.medium.com/max/1600/1*1woqrqfRwmS1xXYHKPMUDw.png)](https://buzzrobot.com/bias-and-variance-11d8e1fee627)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, neural networks tends to overfit easily. In order to avoid that, we must use a proper <b>regularization</b> method. With a fine-tuned model we could overcome this problem and significantly reduce the error.\n",
    "\n",
    "The most common methods are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>penalization of the coefficients</b>:\n",
    "\n",
    "One of the most common methods of regularization; It limit the capacity of the model to overfit by adding a parameter norm penalty in a form: \n",
    "     \n",
    "$\\tilde{J}(\\theta) = J(\\theta) + \\alpha\\Omega(\\theta)$\n",
    "\n",
    "Two most common methods are:\n",
    "- $\\Omega(\\theta) = ||\\theta||_1 = \\sum_i{|\\theta_i|}$     (<i>LASSO</i>, <i>$L_1$ regularization</i>)\n",
    "- $\\Omega(\\theta) = ||\\theta||_2^2 = \\sum_i{\\theta_i^2}$ (<i>Tikhonov regularization</i>, <i>Ridge regression</i>, <i>$L_2$ regularization</i>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They implementation is [as follows](https://fluxml.ai/Flux.jl/stable/models/regularisation/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L₁(θ) = sum(abs, θ) \n",
    "L₂(θ) = sum(abs2, θ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J(x,y,W) = loss(model(x),y) + L₁(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J(x,y,W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Bagging (bootstrap aggregating)</b>:\n",
    "\n",
    "From the initial dataset, $k$ sets are sampled uniformly with replacement. Then $k$  models are trained using the above sets. Final result is obtained by aggregating the output of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Dropout</b>:\n",
    "\n",
    "In every iteration of the training, neurons are removed from the model with a probability $p$. Binary vector $\\mu = [1,1,0,1,1,1,\\dots,0,1]$ represent the neurons used in the training process in $i$-th iteration. The goal of the training process is to minimize the value of $E_\\mu[J(\\theta,\\mu)]$ for every iteration. As a result, we obtain the unbiased estimator of a gradient function without the costly process of generating and training $k$ separated models.\n",
    "\n",
    "Dropout is implemented as a [layer of model](https://fluxml.ai/Flux.jl/stable/models/layers/#Normalisation-and-Regularisation-1): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain(Dense(28^2, 32, relu),\n",
    "    Dropout(0.1),\n",
    "Dense(32, 10),\n",
    "BatchNorm(64, relu),\n",
    "softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Goodfellow I., Bengio Y., Courville A. (2016), Deep Learning, Chapter 8](http://www.deeplearningbook.org/contents/optimization.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choice of the proper optimization algorithm is the most important step during the neural network training. Cost function minimization is non-trivial task; there are plenty of serious problems which might derail the learning process:\n",
    "- ill-conditioned Hessian matrix.\n",
    "- local minimas, plateaus, etc.\n",
    "- vanishing and exploding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result vanilla stochastic gradient method might not be able to solve such problem. There are many modifications of SGD algorithm introduced to overcome these issues:\n",
    "- SGD [(Robbins & Munro 1951)](https://projecteuclid.org/download/pdf_1/euclid.aoms/1177729586)\n",
    "- SGD with momentum [(Polyak, 1964)](http://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=zvmmf&paperid=7713&option_lang=eng)\n",
    "- SGD with  Nesterov momentum ([Nesterov, 1983](http://www.cis.pku.edu.cn/faculty/vision/zlin/1983-A%20Method%20of%20Solving%20a%20Convex%20Programming%20Problem%20with%20Convergence%20Rate%20O%28k%5E%28-2%29%29_Nesterov.pdf), [2005](https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf))\n",
    "- AdaGrad (Adaptive Gradient Algorithm) [(Duchi et. al. 2011)](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n",
    "- ADAM (Adaptive Moment Estimation) [(Kingma & Ba, 2015)](https://arxiv.org/abs/1412.6980)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flux allows to [calculate a gradient of every function](https://fluxml.ai/Flux.jl/stable/models/basics/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = 3x^2 + 2x + 1\n",
    "\n",
    "# df/dx = 6x + 2\n",
    "df(x) = gradient(f, x)[1]\n",
    "\n",
    "df(2) # 14.0 \n",
    "\n",
    "# d²f/dx² = 6\n",
    "d²f(x) = gradient(df, x)[1]\n",
    "\n",
    "d²f(2) # 6.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if the function is not declared as a mathematical formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function pow(x, n)\n",
    "    r = 1\n",
    "    for i = 1:n\n",
    "        r *= x\n",
    "    end\n",
    "    return r\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow(2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient(x -> pow(x, 3), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow2(x, n) = n <= 0 ? 1 : x*pow2(x, n-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient(x -> pow2(x, 3), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible because of the efficient differentiation algorithm implemented in the [<tt>Zygote.jl</tt>](https://fluxml.ai/Zygote.jl/latest/) package. It uses the characteristic elements of the language, e.g. its compiler to find the derivatives of functions. A brief explanation how <tt>Zygote</tt> works is avalaible [here](https://github.com/MikeInnes/diff-zoo) and [here](https://arxiv.org/pdf/1810.07951.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key element of the neural networks training is finding a gradients. As we now from the previous lecture, naïve numerical approximation of derivatives by the definition:\n",
    "$$\\frac{df}{dx} = \\lim_{h \\to 0}\\frac{f(x_0 +h) - f(x_0)}{h}$$\n",
    "is a recipe for disaster. So, how we could find the derivatives on the computer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out, that even the most complicated function could be represented as a composition of some basic functions (sin,cos,log,etc.) and arithmetic operations. Knowing the derivatives of these basic functions, we could compute basically every derivative by using a <i>chain rule</i>:\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{dy_1}{dx}*\\frac{dy_2}{dy_1}*\\dots*\\frac{dy_{n-1}}{dy_{n-2}}*\\frac{dy}{dy_{n-1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two distinct methods of automatic differentiation:\n",
    "\n",
    "- <b>Forward Accumulation</b> we start with a known value of $\\frac{dy_0}{dx} = \\frac{dx}{dx} = 1$. then we are computing the value of the next step $\\frac{dy_1}{dx} = \\frac{d_1}{dx} = 1$ and we continue this process, calculating the values of next elements of chain:$\\frac{dy_{i+1}}{dy_i}$, until we reach a final step.\n",
    "\n",
    "- <b>Backward Accumulation</b> we start with a known value of  $\\frac{dy}{dy_n} = \\frac{dy}{dy} = 1$, then we are finding the value of: $\\frac{dy}{dy_n}$, $\\frac{dy}{dy_{n-1}}$, ... $\\frac{dy}{dy_1}$, $\\frac{dy}{dx}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zygote.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tt>Zygote</tt> package is based around two crucial elements: macro <tt>@adjoint</tt> and function <tt>pullback</tt>. \n",
    "\n",
    "<tt>pullback</tt> returns two things, the value of original function $y = f(x)$ and <i>pullback</i> expression $ \\mathcal{B}(\\overline{y}) = \\overline{y}  \\frac{dy}{dx}$, where $\\overline{y} = \\frac{dl}{dy}$ is a predefined parameter for a given function $l$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Zygote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, back = Zygote.pullback(sin, π);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2246467991473532e-16"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sin(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, for function <tt>gradient</tt>  $l = y = f(x)$ and  $\\overline{y} = \\frac{dy}{dl} = 1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient(sin,π) == back(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Macro <tt>@adjoint</tt> allows us to compute custom adjoints and modify the derivation mechanism:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Zygote: @adjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minus(a,b) = a - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient(minus,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minus2(a,b) = a - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@adjoint minus2(a,b) = minus2(a,b), c̄ -> (nothing, -b^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient(minus2,2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Obviously Flux have implemented [the most common optimization algorithms](https://fluxml.ai/Flux.jl/stable/training/optimisers/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = ADAM(0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Flux can control the entire learning process with a function <tt>train!</tt>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.train!(objective, data, opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, <tt>train!</tt> works for only one epoch. If we need to train the model for more than one epoch, we must either modify the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Base.Iterators: repeated\n",
    "dataset = repeated((x, y), 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or use the macro <tt>@epochs</tt>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.@epochs 2 println(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In flux we could also define the callbacks, which will help us control the learning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalcb = () -> @show(loss(tX, tY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Flux.Data.MNIST, Statistics\n",
    "using Flux: onehotbatch, onecold, crossentropy, throttle\n",
    "using Base.Iterators: repeated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to wrap everything up. Let start with preparing a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Classify MNIST digits with a simple multi-layer-perceptron\n",
    "\n",
    "imgs = MNIST.images()\n",
    "# Stack images into one large batch\n",
    "X = hcat(float.(reshape.(imgs, :))...) \n",
    "\n",
    "labels = MNIST.labels()\n",
    "# One-hot-encode the labels\n",
    "Y = onehotbatch(labels, 0:9) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we must define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Chain(\n",
    "  Dense(28^2, 32, relu),\n",
    "  Dense(32, 10),\n",
    "  softmax) \n",
    "\n",
    "loss(x, y) = crossentropy(m(x), y)\n",
    "\n",
    "accuracy(x, y) = mean(onecold(m(x)) .== onecold(y))\n",
    "\n",
    "dataset = repeated((X, Y), 200)\n",
    "evalcb = () -> @show(loss(X, Y))\n",
    "opt = ADAM()\n",
    "\n",
    "\n",
    "accuracy(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time for some training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.train!(loss, params(m), dataset, opt, cb = throttle(evalcb, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could save results in the [<tt>BSON</tt>](https://github.com/JuliaIO/BSON.jl) format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSON.@save \"MNIST.bson\" m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and obviosly load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSON.@load \"MNIST.bson\" m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we could take the look at the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set accuracy\n",
    "tX = hcat(float.(reshape.(MNIST.images(:test), :))...) \n",
    "tY = onehotbatch(MNIST.labels(:test), 0:9) \n",
    "\n",
    "accuracy(tX, tY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training process, only the weights $\\theta$ are optimized. All the other parameters of the neural network (activation functions, regularization methods, optimization algorithm and its parameters, learning rate, etc.) are predefined. In order to find the best model, we must carefully choose them. We might do this with a plenty of different methods:\n",
    "- using the examples from the literature\n",
    "- randomly generating and comparing different combinations of hyperparameters\n",
    "- building a model able to [learn the best parameters for our task](https://arxiv.org/abs/2004.05439)\n",
    "- searching the hyperparameters space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. Tune the hyperparameters of the network presented during the lecture; try to find the combination which will give you the highest accuracy <b>(5 points)</b>.\n",
    "2. During the lecture we indtroduce the <i>dual numbers</i>.We define them as expressions in the form: $z = a + \\epsilon b$, where $a,b \\in \\mathbb{R}$ and  $\\epsilon^2 = 0$. For every polynomial: $f(x) = a_0 + a_1x + a_2x^2 + \\dots + a_nx^n$ its value for the dual number $z$ is equal to: $f(z) = f(a) + bf'(a)\\epsilon$, prove it <b>(5 points)</b>."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
